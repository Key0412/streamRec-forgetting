{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run_experiment(\"configuration/file/path\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-08 16:35:49.431064: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\n",
      "2023-03-08 16:35:49.842468: E tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-03-08 16:35:49.842555: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties: \n",
      "pciBusID: 0000:01:00.0 name: NVIDIA GeForce RTX 3060 Laptop GPU computeCapability: 8.6\n",
      "coreClock: 1.702GHz coreCount: 30 deviceMemorySize: 6.00GiB deviceMemoryBandwidth: 312.97GiB/s\n",
      "2023-03-08 16:35:49.844220: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
      "2023-03-08 16:35:49.859548: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n",
      "2023-03-08 16:35:49.873917: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n",
      "2023-03-08 16:35:49.877326: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n",
      "2023-03-08 16:35:49.892920: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n",
      "2023-03-08 16:35:49.897638: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n",
      "2023-03-08 16:35:49.931032: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
      "2023-03-08 16:35:49.931609: E tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-03-08 16:35:49.931907: E tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-03-08 16:"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "35:49.931952: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0\n",
      "2023-03-08 16:35:49.934218: I tensorflow/core/platform/cpu_feature_guard.cc:143] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "2023-03-08 16:35:49.946273: I tensorflow/core/platform/profile_utils/cpu_utils.cc:102] CPU Frequency: 2208005000 Hz\n",
      "2023-03-08 16:35:49.950128: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7f45ec000b60 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2023-03-08 16:35:49.950156: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2023-03-08 16:35:50.610612: E tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-03-08 16:35:50.610763: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x2eb4160 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2023-03-08 16:35:50.610777: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce RTX 3060 Laptop GPU, Compute Capability 8.6\n",
      "2023-03-08 16:35:50.611274: E tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-03-08 16:35:50.611295: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties: \n",
      "pciBusID: 0000:01:00.0 name: NVIDIA GeForce RTX 3060 Laptop GPU computeCapability: 8.6\n",
      "coreClock: 1.702GHz coreCount: 30 deviceMemorySize: 6.00GiB deviceMemoryBandwidth: 312.97GiB/s\n",
      "2023-03-08 16:35:50.611334: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
      "2023-03-08 16:35:50.611345: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n",
      "2023-03-08 16:35:50.611356: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n",
      "2023-03-08 16:35:50.611365: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n",
      "2023-03-08 16:35:50.611373: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n",
      "2023-03-08 16:35:50.611382: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n",
      "2023-03-08 16:35:50.611391: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
      "2023-03-08 16:35:50.611577: E tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-03-08 16:35:50.611771: E tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-03-08 16:35:50.611783: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0\n",
      "2023-03-08 16:35:50.611873: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
      "2023-03-08 16:35:50.612249: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2023-03-08 16:35:50.612260: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1108]      0 \n",
      "2023-03-08 16:35:50.612264: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1121] 0:   N \n",
      "2023-03-08 16:35:50.613422: E tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-03-08 16:35:50.613435: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1330] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2023-03-08 16:35:50.613631: E tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-03-08 16:35:50.613728: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1247] Created TensorFlow device (/device:GPU:0 with 4826 MB memory) -> physical GPU (device: 0, name: NVIDIA GeForce RTX 3060 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[name: \"/device:CPU:0\"\n",
       " device_type: \"CPU\"\n",
       " memory_limit: 268435456\n",
       " locality {\n",
       " }\n",
       " incarnation: 4760860461784065945,\n",
       " name: \"/device:XLA_CPU:0\"\n",
       " device_type: \"XLA_CPU\"\n",
       " memory_limit: 17179869184\n",
       " locality {\n",
       " }\n",
       " incarnation: 11741707862507982599\n",
       " physical_device_desc: \"device: XLA_CPU device\",\n",
       " name: \"/device:XLA_GPU:0\"\n",
       " device_type: \"XLA_GPU\"\n",
       " memory_limit: 17179869184\n",
       " locality {\n",
       " }\n",
       " incarnation: 2432709613630644377\n",
       " physical_device_desc: \"device: XLA_GPU device\",\n",
       " name: \"/device:GPU:0\"\n",
       " device_type: \"GPU\"\n",
       " memory_limit: 5061371488\n",
       " locality {\n",
       "   bus_id: 1\n",
       "   links {\n",
       "   }\n",
       " }\n",
       " incarnation: 8891658831692616450\n",
       " physical_device_desc: \"device: 0, name: NVIDIA GeForce RTX 3060 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\"]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python.client import device_lib\n",
    "tf.autograph.set_verbosity(5)\n",
    "\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "device_lib.list_local_devices()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.abspath('') + '/../../..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data import ImplicitData, getBucketsHoldouts\n",
    "from plot_utils import lineplot_recallxholdout, recall_heatmap\n",
    "from dataset_evaluation_utils import *\n",
    "from recommenders_implicit import ISGD, RAISGD, RSISGD  # ISGD framework, BISGD,\n",
    "from eval_implicit import EvaluateHoldouts, EvaluateAndStore, EvalPrequential # EvaluateAndStore para guardar estados do modelo e holdouts, a avaliação prequencial de ratings implicitos é opcional, , EvalHoldout\n",
    "\n",
    "from datetime import datetime\n",
    "import joblib\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "sns.set_style('whitegrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "# BWT FWT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ACC, BWT, e FWT - Lopez-Paz e Ranzato GEM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_recall(results_matrix): # Lopez-Paz e Ranzato GEM 2017\n",
    "    return np.mean( np.diag(results_matrix) )\n",
    "\n",
    "def compute_BWT(results_matrix): # Lopez-Paz e Ranzato GEM 2017\n",
    "    BWT = []\n",
    "    n_checkpoints = results_matrix.shape[0]\n",
    "    for T in range(1, n_checkpoints): # 1 means holdout 2, 2 means 3, so on\n",
    "        Rti = results_matrix.iloc[T, 0:T] # get models performances' on previous holdouts\n",
    "        Rii = np.diag(results_matrix)[0:T] # get models performances' on their closest holdouts (diagonal)\n",
    "        E = sum( Rti - Rii ) # future models performances' - performances' of models closest to holdouts (diagonal)\n",
    "        BWT.append( E/T ) # store average BWT for model\n",
    "    return BWT, np.mean( BWT ) # return BWT and average BWT for all models\n",
    "\n",
    "def compute_FWT(results_matrix): # Díaz-Rodriguez et al. 2018\n",
    "    upper_tri = results_matrix.to_numpy()[np.triu_indices(results_matrix.shape[0], k=1)]\n",
    "    return np.mean(upper_tri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "# Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importa dataset 'movieles'\n",
    "data = pd.read_csv('../../output/movielens_dump/sampled_movielens.csv')\n",
    "user_col = 'UserID'\n",
    "item_col = 'ItemID'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((50742, 7), 1427, 2492)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape, data[user_col].nunique(), data[item_col].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[[user_col, item_col]].duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UserID</th>\n",
       "      <th>ItemID</th>\n",
       "      <th>Timestamp</th>\n",
       "      <th>date2</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4448</td>\n",
       "      <td>902</td>\n",
       "      <td>965087178</td>\n",
       "      <td>2000-07-31 23:46:18</td>\n",
       "      <td>2000</td>\n",
       "      <td>7</td>\n",
       "      <td>2000-07-01 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4448</td>\n",
       "      <td>3793</td>\n",
       "      <td>965087267</td>\n",
       "      <td>2000-07-31 23:47:47</td>\n",
       "      <td>2000</td>\n",
       "      <td>7</td>\n",
       "      <td>2000-07-01 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4448</td>\n",
       "      <td>3751</td>\n",
       "      <td>965087267</td>\n",
       "      <td>2000-07-31 23:47:47</td>\n",
       "      <td>2000</td>\n",
       "      <td>7</td>\n",
       "      <td>2000-07-01 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4448</td>\n",
       "      <td>3578</td>\n",
       "      <td>965087349</td>\n",
       "      <td>2000-07-31 23:49:09</td>\n",
       "      <td>2000</td>\n",
       "      <td>7</td>\n",
       "      <td>2000-07-01 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4448</td>\n",
       "      <td>3481</td>\n",
       "      <td>965087470</td>\n",
       "      <td>2000-07-31 23:51:10</td>\n",
       "      <td>2000</td>\n",
       "      <td>7</td>\n",
       "      <td>2000-07-01 00:00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   UserID  ItemID  Timestamp                date2  year  month  \\\n",
       "0    4448     902  965087178  2000-07-31 23:46:18  2000      7   \n",
       "1    4448    3793  965087267  2000-07-31 23:47:47  2000      7   \n",
       "2    4448    3751  965087267  2000-07-31 23:47:47  2000      7   \n",
       "3    4448    3578  965087349  2000-07-31 23:49:09  2000      7   \n",
       "4    4448    3481  965087470  2000-07-31 23:51:10  2000      7   \n",
       "\n",
       "                  date  \n",
       "0  2000-07-01 00:00:00  \n",
       "1  2000-07-01 00:00:00  \n",
       "2  2000-07-01 00:00:00  \n",
       "3  2000-07-01 00:00:00  \n",
       "4  2000-07-01 00:00:00  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 50742 entries, 0 to 50741\n",
      "Data columns (total 7 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   UserID     50742 non-null  int64 \n",
      " 1   ItemID     50742 non-null  int64 \n",
      " 2   Timestamp  50742 non-null  int64 \n",
      " 3   date2      50742 non-null  object\n",
      " 4   year       50742 non-null  int64 \n",
      " 5   month      50742 non-null  int64 \n",
      " 6   date       50742 non-null  object\n",
      "dtypes: int64(5), object(2)\n",
      "memory usage: 2.7+ MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 452 ms, sys: 3.59 ms, total: 455 ms\n",
      "Wall time: 454 ms\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "# 2.42s\n",
    "data['date'] = data['date'].apply(lambda x: datetime.strptime(x, '%Y-%m-%d %X'))\n",
    "# data.sort_values(by='timestamp', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXPERIMENT 7\n",
    "\n",
    "CODE TO GET LAST N INTERACTIONS FROM EACH USER AS HOLDOUT  \n",
    "* IF USER DID NOT INTERACT WITH AT LEAST N+1 ITEMS, THEN IT IS NOT USED FOR HOLDOUT  \n",
    "* LAST 10 INTERACTIONS FROM EACH USER AS HOLDOUT\n",
    "* RECOMENDING SEEN ITEMS IS ALLOWED\n",
    "* DON'T REMOVE INTERACTIONS IN HOLDOUT FROM BUCKETS\n",
    "* DON'T REMOVE INTERACTIONS **SENT** TO HOLDOUT FROM BUCKET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating buckets. . .\n"
     ]
    }
   ],
   "source": [
    "# CODE TO GET LAST N INTERACTIONS FROM EACH USER AS HOLDOUT\n",
    "# IF USER DID NOT INTERACT WITH AT LEAST N+1 ITEMS, THEN IT IS NOT USED FOR HOLDOUT\n",
    "\n",
    "N = 10\n",
    "cold_start_buckets = 0\n",
    "#     print('0',data.shape[0]) # debug\n",
    "print('Creating buckets. . .')\n",
    "buckets = []\n",
    "# assert interval_type in ['W', 'M', 'QS', 'F'], \"interval must be one of W, M, QS, or F\"\n",
    "# create buckets based on months\n",
    "months = data['date'].unique()\n",
    "months.sort()\n",
    "for interval in months:\n",
    "    idx = (data['date'] == interval)\n",
    "    buckets.append( data[idx] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating holdouts. . .\n"
     ]
    }
   ],
   "source": [
    "print('Creating holdouts. . .')\n",
    "# create holdouts with last user interaction\n",
    "holdouts = []\n",
    "\n",
    "for i, b in enumerate( buckets ):\n",
    "    if i >= cold_start_buckets:\n",
    "        condition = (b[user_col].value_counts() > N)\n",
    "        frequent_users = b[user_col].value_counts()[ condition ].index\n",
    "        holdout_idx = []\n",
    "        for u in frequent_users:\n",
    "            tail_idx = list( b[b[user_col] == u].tail(N).index )\n",
    "            holdout_idx += tail_idx\n",
    "        holdout = b.loc[holdout_idx].reset_index(drop=True)\n",
    "        holdouts.append(holdout)\n",
    "        # buckets[i] = b.drop(index=holdout_idx).reset_index(drop=True)\n",
    "        buckets[i] = b.reset_index(drop=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Store buckets and holdouts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "buckets[0].to_csv('movielens_bucket_0.csv', columns=[user_col, item_col], header=False, index=False)\n",
    "holdouts[0].to_csv('movielens_holdout_0.csv', columns=[user_col, item_col], header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16617, 7)\n",
      "(13148, 7)\n",
      "(20977, 7)\n"
     ]
    }
   ],
   "source": [
    "for b in buckets:\n",
    "    print(b.shape)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3710, 7)\n",
      "(3240, 7)\n",
      "(5040, 7)\n"
     ]
    }
   ],
   "source": [
    "for h in holdouts:\n",
    "    print(h.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "holdouts[1].to_csv('movielens_holdout_1.csv', columns=[user_col, item_col], header=False, index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running experiment with base configuration - there is optimization and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "__/\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\___/\\\\\\\\\\\\______/\\\\\\\\\\\\_________________________________________        \n",
      " _\\/\\\\\\///////////___\\////\\\\\\_____\\////\\\\\\_________________________________________       \n",
      "  _\\/\\\\\\_________________\\/\\\\\\________\\/\\\\\\______/\\\\\\_____________________/\\\\\\______      \n",
      "   _\\/\\\\\\\\\\\\\\\\\\\\\\_________\\/\\\\\\________\\/\\\\\\_____\\///_______/\\\\\\\\\\______/\\\\\\\\\\\\\\\\\\\\\\_     \n",
      "    _\\/\\\\\\///////__________\\/\\\\\\________\\/\\\\\\______/\\\\\\____/\\\\\\///\\\\\\___\\////\\\\\\////__    \n",
      "     _\\/\\\\\\_________________\\/\\\\\\________\\/\\\\\\_____\\/\\\\\\___/\\\\\\__\\//\\\\\\_____\\/\\\\\\______   \n",
      "      _\\/\\\\\\_________________\\/\\\\\\________\\/\\\\\\_____\\/\\\\\\__\\//\\\\\\__/\\\\\\______\\/\\\\\\_/\\\\__  \n",
      "       _\\/\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\___/\\\\\\\\\\\\\\\\\\___/\\\\\\\\\\\\\\\\\\__\\/\\\\\\___\\///\\\\\\\\\\/_______\\//\\\\\\\\\\___ \n",
      "        _\\///////////////___\\/////////___\\/////////___\\///______\\/////__________\\/////____\n",
      "Version Number: 0.3.1\n"
     ]
    }
   ],
   "source": [
    "from elliot.run import run_experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-03-08 16:35:53.487005: I Start experiment\n",
      "2023-03-08 16:35:53.520819: I /home/kpfra/streamRec-forgetting/notebooks/elliot_experiments/elliot_example/movielens_bucket_0.csv - Loaded\n",
      "2023-03-08 16:35:53.539036: I Test Fold 0\n",
      "2023-03-08 16:36:39.980555: I Statistics\tUsers:\t16617\tItems:\t16617\tTransactions:\t16617\tSparsity:\t0.9999398206655834\n",
      "2023-03-08 16:36:55.294315: I Tuning begun for MultiVAE\\n\n",
      "2023-03-08 16:36:55.297764: I Loading parameters\n",
      "2023-03-08 16:36:55.298160: I Hyperparameter tuning exploration:\n",
      "2023-03-08 16:36:55.300570: I batch_size set to 17\n",
      "2023-03-08 16:36:55.300749: I Parameter intermediate_dim set to 155\n",
      "2023-03-08 16:36:55.302819: I dropout_pkeep set to 0.25040322158625117\n",
      "2023-03-08 16:36:55.302660: I Parameter latent_dim set to 101\n",
      "2023-03-08 16:36:55.303842: I Parameter reg_lambda set to 0.021991645792416692\n",
      "2023-03-08 16:36:55.304665: I epochs set to 10\n",
      "2023-03-08 16:36:55.305581: I intermediate_dim set to 155\n",
      "2023-03-08 16:36:55.305451: I Parameter lr set to 0.30987825830054144\n",
      "2023-03-08 16:36:55.307362: I latent_dim set to 101\n",
      "2023-03-08 16:36:55.307194: I Parameter dropout_pkeep set to 0.25040322158625117\n",
      "2023-03-08 16:36:55.309449: I lr set to 0.30987825830054144\n",
      "2023-03-08 16:36:55.310871: I reg_lambda set to 0.021991645792416692\n",
      "2023-03-08 16:36:55.312036: I Exploration: Hyperparameter exploration number 1\n",
      "2023-03-08 16:36:55.313403: I Exploration: Test Fold exploration number 1\n",
      "2023-03-08 16:36:55.323233: I Exploration: Train-Validation Fold exploration number 1\n",
      "!!!!!!!!!!!!!!!!!!\n",
      "PASSED TO TRAIN LOOP\n",
      "!!!!!!!!!!!!!!!!!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-08 16:36:55.349407: E tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-03-08 16:36:55.349485: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties: \n",
      "pciBusID: 0000:01:00.0 name: NVIDIA GeForce RTX 3060 Laptop GPU computeCapability: 8.6\n",
      "coreClock: 1.702GHz coreCount: 30 deviceMemorySize: 6.00GiB deviceMemoryBandwidth: 312.97GiB/s\n",
      "2023-03-08 16:36:55.349562: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
      "2023-03-08 16:36:55.349578: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n",
      "2023-03-08 16:36:55.349591: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n",
      "2023-03-08 16:36:55.349602: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n",
      "2023-03-08 16:36:55.349614: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n",
      "2023-03-08 16:36:55.349624: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n",
      "2023-03-08 16:36:55.349636: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
      "2023-03-08 16:36:55.350037: E tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-03-08 16:36:55.350438: E tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-03-08 16:36:55.350458: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0\n",
      "2023-03-08 16:36:55.350493: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2023-03-08 16:36:55.350499: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1108]      0 \n",
      "2023-03-08 16:36:55.350503: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1121] 0:   N \n",
      "2023-03-08 16:36:55.351070: E tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-03-08 16:36:55.351086: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1330] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2023-03-08 16:36:55.351403: E tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-03-08 16:36:55.351441: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1247] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 4826 MB memory) -> physical GPU (device: 0, name: NVIDIA GeForce RTX 3060 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6)\n",
      "2023-03-08 16:37:01.605216: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-03-08 16:40:03.724125: I \n",
      "2023-03-08 16:40:03.723981: I Epoch 1/10 loss nan\n",
      "2023-03-08 16:40:03.726155: I ******************************************\n",
      "2023-03-08 16:40:03.726805: I Test Evaluation results\n",
      "2023-03-08 16:40:03.731754: I Cut-off: 20\n",
      "2023-03-08 16:40:03.732932: I Eval Time: 0.14473438262939453\n",
      "2023-03-08 16:40:03.734133: I Results\n",
      "2023-03-08 16:40:03.735347: I nDCG\t0.0\n",
      "2023-03-08 16:40:03.737189: I Precision\t0.0\n",
      "2023-03-08 16:40:03.738687: I Recall\t0.0\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "run_experiment('elliot_example_configuration.yml')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running experiment with RESTORE - model weights are read and there is no training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-03-06 17:45:28.229007: I Start experiment\n",
      "2023-03-06 17:45:28.238097: I /home/kpfra/streamRec-forgetting/notebooks/elliot_experiments/elliot_example/movielens_bucket_1.csv - Loaded\n",
      "2023-03-06 17:45:28.244117: I Test Fold 0\n",
      "2023-03-06 17:45:29.429240: I Statistics\tUsers:\t1000\tItems:\t1000\tTransactions:\t1000\tSparsity:\t0.999\n",
      "2023-03-06 17:45:29.949316: I Training begun for MultiVAE\\n\n",
      "!!!!!!!!!!!!!!!!!!\n",
      "ENTERS RESTORE IF\n",
      "!!!!!!!!!!!!!!!!!!\n",
      "Model correctly Restored\n",
      "2023-03-06 17:45:29.962517: I Hyperparameters:\n",
      "2023-03-06 17:45:29.965603: I meta set to namespace(restore=True, save_recs=False, save_weights=True, validation_metric='Recall@20', verbose=False)\n",
      "2023-03-06 17:45:29.969028: I epochs set to 8\n",
      "2023-03-06 17:45:29.971113: I batch_size set to 470\n",
      "2023-03-06 17:45:29.973375: I intermediate_dim set to 331\n",
      "2023-03-06 17:45:29.977459: I latent_dim set to 493\n",
      "2023-03-06 17:45:29.979519: I mf_factors set to 11\n",
      "2023-03-06 17:45:29.981817: I reg_lambda set to 0.021991645792416692\n",
      "2023-03-06 17:45:29.986219: I lr set to 0.0014786280507223606\n",
      "2023-03-06 17:45:29.988617: I dropout_pkeep set to 0.29826163136513073\n",
      "2023-03-06 17:45:29.990447: I Exploration: Test Fold exploration number 1\n",
      "2023-03-06 17:45:29.991541: I Exploration: Train-Validation Fold exploration number 1\n",
      "2023-03-06 17:45:30.320403: I \n",
      "2023-03-06 17:45:30.323265: I Test Evaluation results\n",
      "2023-03-06 17:45:30.325091: I Cut-off: 20\n",
      "2023-03-06 17:45:30.326693: I Eval Time: 0.0022966861724853516\n",
      "2023-03-06 17:45:30.329339: I Results\n",
      "2023-03-06 17:45:30.332063: I nDCG\t0.0\n",
      "2023-03-06 17:45:30.333253: I Precision\t0.0\n",
      "2023-03-06 17:45:30.334792: I Recall\t0.0\n",
      "2023-03-06 17:45:30.353548: I Training ended for MultiVAE\n",
      "2023-03-06 17:45:30.355260: I Loss:\\t0.0\n",
      "2023-03-06 17:45:30.359392: I Best Model params:\\t{'meta': namespace(restore=True, save_recs=False, save_weights=True, validation_metric='Recall@20', verbose=False), 'epochs': 8, 'batch_size': 470, 'intermediate_dim': 331, 'latent_dim': 493, 'mf_factors': 11, 'reg_lambda': 0.021991645792416692, 'lr': 0.0014786280507223606, 'dropout_pkeep': 0.29826163136513073, 'name': 'MultiVAE_seed=42_e=8_bs=470_intermediate_dim=331_latent_dim=493_reg_lambda=0$021991645792416692_lr=0$0014786280507223606_dropout_pkeep=0$7017383686348693'}\n",
      "2023-03-06 17:45:30.362856: I Best Model results:\\t{20: {'nDCG': 0.0, 'Precision': 0.0, 'Recall': 0.0}}\n",
      "2023-03-06 17:45:30.365177: I End experiment\n"
     ]
    }
   ],
   "source": [
    "run_experiment('elliot_example_configuration Load Test.yml')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running experiment with RESTORE, but with ranges defined for the optimization\n",
    "* This does not work, because the framework tries to find a folder from a previous model that contains in its name the same hyperparameters it is running with in an iteration.\n",
    "* This means datasets must be stored in versions that contain an increasing number of buckets and holdouts:  \n",
    "    * b0.csv - h0.csv\n",
    "    * b0_b1.csv - h1.csv\n",
    "    * b0_b1_b2.csv - h2.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-03-06 17:49:27.277543: I Start experiment\n",
      "2023-03-06 17:49:27.285791: I /home/kpfra/streamRec-forgetting/notebooks/elliot_experiments/elliot_example/movielens_bucket_1.csv - Loaded\n",
      "2023-03-06 17:49:27.293861: I Test Fold 0\n",
      "2023-03-06 17:49:28.459648: I Statistics\tUsers:\t1000\tItems:\t1000\tTransactions:\t1000\tSparsity:\t0.999\n",
      "2023-03-06 17:49:28.956628: I Tuning begun for MultiVAE\\n\n",
      "!!!!!!!!!!!!!!!!!!\n",
      "ENTERS RESTORE IF\n",
      "!!!!!!!!!!!!!!!!!!\n",
      "2023-03-06 17:49:28.981404: I Hyperparameter tuning exploration:\n",
      "2023-03-06 17:49:28.987666: I batch_size set to 928\n",
      "2023-03-06 17:49:29.036221: I dropout_pkeep set to 0.6342344071271315\n",
      "2023-03-06 17:49:29.078902: I epochs set to 3\n",
      "2023-03-06 17:49:29.085776: I intermediate_dim set to 141\n",
      "2023-03-06 17:49:29.097875: I latent_dim set to 304\n",
      "2023-03-06 17:49:29.100747: I lr set to 0.09139786666274591\n",
      "2023-03-06 17:49:29.108933: I mf_factors set to 11\n",
      "2023-03-06 17:49:29.143828: I reg_lambda set to 0.4291070790802781\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "Error in model restoring operation! Unsuccessful TensorSliceReader constructor: Failed to find any matching files for /home/kpfra/streamRec-forgetting/notebooks/elliot_experiments/elliot_example/results/weights/MultiVAE_seed=42_e=3_bs=928_intermediate_dim=141_latent_dim=304_reg_lambda=0$4291070790802781_lr=0$09139786666274591_dropout_pkeep=0$3657655928728685/best-weights-MultiVAE_seed=42_e=3_bs=928_intermediate_dim=141_latent_dim=304_reg_lambda=0$4291070790802781_lr=0$09139786666274591_dropout_pkeep=0$3657655928728685",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m~/.virtualenvs/dissertacao/lib/python3.8/site-packages/tensorflow/python/training/py_checkpoint_reader.py:95\u001b[0m, in \u001b[0;36mNewCheckpointReader\u001b[0;34m(filepattern)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 95\u001b[0m   \u001b[39mreturn\u001b[39;00m CheckpointReader(compat\u001b[39m.\u001b[39;49mas_bytes(filepattern))\n\u001b[1;32m     96\u001b[0m \u001b[39m# TODO(b/143319754): Remove the RuntimeError casting logic once we resolve the\u001b[39;00m\n\u001b[1;32m     97\u001b[0m \u001b[39m# issue with throwing python exceptions from C++.\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Unsuccessful TensorSliceReader constructor: Failed to find any matching files for /home/kpfra/streamRec-forgetting/notebooks/elliot_experiments/elliot_example/results/weights/MultiVAE_seed=42_e=3_bs=928_intermediate_dim=141_latent_dim=304_reg_lambda=0$4291070790802781_lr=0$09139786666274591_dropout_pkeep=0$3657655928728685/best-weights-MultiVAE_seed=42_e=3_bs=928_intermediate_dim=141_latent_dim=304_reg_lambda=0$4291070790802781_lr=0$09139786666274591_dropout_pkeep=0$3657655928728685",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[0;32m~/elliot/elliot/recommender/recommender_utils_mixin.py:101\u001b[0m, in \u001b[0;36mRecMixin.restore_weights\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 101\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_model\u001b[39m.\u001b[39;49mload_weights(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_saving_filepath)\n\u001b[1;32m    102\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mModel correctly Restored\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.virtualenvs/dissertacao/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:250\u001b[0m, in \u001b[0;36mModel.load_weights\u001b[0;34m(self, filepath, by_name, skip_mismatch)\u001b[0m\n\u001b[1;32m    248\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mLoad weights is not yet supported with TPUStrategy \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    249\u001b[0m                      \u001b[39m'\u001b[39m\u001b[39mwith steps_per_run greater than 1.\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m--> 250\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m(Model, \u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39;49mload_weights(filepath, by_name, skip_mismatch)\n",
      "File \u001b[0;32m~/.virtualenvs/dissertacao/lib/python3.8/site-packages/tensorflow/python/keras/engine/network.py:1231\u001b[0m, in \u001b[0;36mNetwork.load_weights\u001b[0;34m(self, filepath, by_name, skip_mismatch)\u001b[0m\n\u001b[1;32m   1230\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1231\u001b[0m   py_checkpoint_reader\u001b[39m.\u001b[39;49mNewCheckpointReader(filepath)\n\u001b[1;32m   1232\u001b[0m   save_format \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mtf\u001b[39m\u001b[39m'\u001b[39m\n",
      "File \u001b[0;32m~/.virtualenvs/dissertacao/lib/python3.8/site-packages/tensorflow/python/training/py_checkpoint_reader.py:99\u001b[0m, in \u001b[0;36mNewCheckpointReader\u001b[0;34m(filepattern)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m---> 99\u001b[0m   error_translator(e)\n",
      "File \u001b[0;32m~/.virtualenvs/dissertacao/lib/python3.8/site-packages/tensorflow/python/training/py_checkpoint_reader.py:35\u001b[0m, in \u001b[0;36merror_translator\u001b[0;34m(e)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mnot found in checkpoint\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m error_message \u001b[39mor\u001b[39;00m (\n\u001b[1;32m     33\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mFailed to find any \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m     34\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mmatching files for\u001b[39m\u001b[39m'\u001b[39m) \u001b[39min\u001b[39;00m error_message:\n\u001b[0;32m---> 35\u001b[0m   \u001b[39mraise\u001b[39;00m errors_impl\u001b[39m.\u001b[39mNotFoundError(\u001b[39mNone\u001b[39;00m, \u001b[39mNone\u001b[39;00m, error_message)\n\u001b[1;32m     36\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mSliced checkpoints are not supported\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m error_message \u001b[39mor\u001b[39;00m (\n\u001b[1;32m     37\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mData type \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m     38\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mnot \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m     39\u001b[0m     \u001b[39m'\u001b[39m\u001b[39msupported\u001b[39m\u001b[39m'\u001b[39m) \u001b[39min\u001b[39;00m error_message:\n",
      "\u001b[0;31mNotFoundError\u001b[0m: Unsuccessful TensorSliceReader constructor: Failed to find any matching files for /home/kpfra/streamRec-forgetting/notebooks/elliot_experiments/elliot_example/results/weights/MultiVAE_seed=42_e=3_bs=928_intermediate_dim=141_latent_dim=304_reg_lambda=0$4291070790802781_lr=0$09139786666274591_dropout_pkeep=0$3657655928728685/best-weights-MultiVAE_seed=42_e=3_bs=928_intermediate_dim=141_latent_dim=304_reg_lambda=0$4291070790802781_lr=0$09139786666274591_dropout_pkeep=0$3657655928728685",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m run_experiment(\u001b[39m'\u001b[39;49m\u001b[39melliot_example_configuration 2.yml\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "File \u001b[0;32m~/elliot/elliot/run.py:82\u001b[0m, in \u001b[0;36mrun_experiment\u001b[0;34m(config_path)\u001b[0m\n\u001b[1;32m     80\u001b[0m logger\u001b[39m.\u001b[39minfo(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mTuning begun for \u001b[39m\u001b[39m{\u001b[39;00mmodel_class\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\\\\\u001b[39;00m\u001b[39mn\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     81\u001b[0m trials \u001b[39m=\u001b[39m Trials()\n\u001b[0;32m---> 82\u001b[0m fmin(model_placeholder\u001b[39m.\u001b[39;49mobjective,\n\u001b[1;32m     83\u001b[0m      space\u001b[39m=\u001b[39;49mmodel_base[\u001b[39m1\u001b[39;49m],\n\u001b[1;32m     84\u001b[0m      algo\u001b[39m=\u001b[39;49mmodel_base[\u001b[39m3\u001b[39;49m],\n\u001b[1;32m     85\u001b[0m      trials\u001b[39m=\u001b[39;49mtrials,\n\u001b[1;32m     86\u001b[0m      verbose\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m     87\u001b[0m      rstate\u001b[39m=\u001b[39;49m_rstate,\n\u001b[1;32m     88\u001b[0m      max_evals\u001b[39m=\u001b[39;49mmodel_base[\u001b[39m2\u001b[39;49m])\n\u001b[1;32m     90\u001b[0m \u001b[39m# argmin relativo alla combinazione migliore di iperparametri\u001b[39;00m\n\u001b[1;32m     91\u001b[0m min_val \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39margmin([i[\u001b[39m\"\u001b[39m\u001b[39mresult\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mloss\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m trials\u001b[39m.\u001b[39m_trials])\n",
      "File \u001b[0;32m~/.virtualenvs/dissertacao/lib/python3.8/site-packages/hyperopt/fmin.py:507\u001b[0m, in \u001b[0;36mfmin\u001b[0;34m(fn, space, algo, max_evals, timeout, loss_threshold, trials, rstate, allow_trials_fmin, pass_expr_memo_ctrl, catch_eval_exceptions, verbose, return_argmin, points_to_evaluate, max_queue_len, show_progressbar, early_stop_fn, trials_save_file)\u001b[0m\n\u001b[1;32m    504\u001b[0m validate_loss_threshold(loss_threshold)\n\u001b[1;32m    506\u001b[0m \u001b[39mif\u001b[39;00m allow_trials_fmin \u001b[39mand\u001b[39;00m \u001b[39mhasattr\u001b[39m(trials, \u001b[39m\"\u001b[39m\u001b[39mfmin\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m--> 507\u001b[0m     \u001b[39mreturn\u001b[39;00m trials\u001b[39m.\u001b[39;49mfmin(\n\u001b[1;32m    508\u001b[0m         fn,\n\u001b[1;32m    509\u001b[0m         space,\n\u001b[1;32m    510\u001b[0m         algo\u001b[39m=\u001b[39;49malgo,\n\u001b[1;32m    511\u001b[0m         max_evals\u001b[39m=\u001b[39;49mmax_evals,\n\u001b[1;32m    512\u001b[0m         timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m    513\u001b[0m         loss_threshold\u001b[39m=\u001b[39;49mloss_threshold,\n\u001b[1;32m    514\u001b[0m         max_queue_len\u001b[39m=\u001b[39;49mmax_queue_len,\n\u001b[1;32m    515\u001b[0m         rstate\u001b[39m=\u001b[39;49mrstate,\n\u001b[1;32m    516\u001b[0m         pass_expr_memo_ctrl\u001b[39m=\u001b[39;49mpass_expr_memo_ctrl,\n\u001b[1;32m    517\u001b[0m         verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[1;32m    518\u001b[0m         catch_eval_exceptions\u001b[39m=\u001b[39;49mcatch_eval_exceptions,\n\u001b[1;32m    519\u001b[0m         return_argmin\u001b[39m=\u001b[39;49mreturn_argmin,\n\u001b[1;32m    520\u001b[0m         show_progressbar\u001b[39m=\u001b[39;49mshow_progressbar,\n\u001b[1;32m    521\u001b[0m         early_stop_fn\u001b[39m=\u001b[39;49mearly_stop_fn,\n\u001b[1;32m    522\u001b[0m         trials_save_file\u001b[39m=\u001b[39;49mtrials_save_file,\n\u001b[1;32m    523\u001b[0m     )\n\u001b[1;32m    525\u001b[0m \u001b[39mif\u001b[39;00m trials \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    526\u001b[0m     \u001b[39mif\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mexists(trials_save_file):\n",
      "File \u001b[0;32m~/.virtualenvs/dissertacao/lib/python3.8/site-packages/hyperopt/base.py:682\u001b[0m, in \u001b[0;36mTrials.fmin\u001b[0;34m(self, fn, space, algo, max_evals, timeout, loss_threshold, max_queue_len, rstate, verbose, pass_expr_memo_ctrl, catch_eval_exceptions, return_argmin, show_progressbar, early_stop_fn, trials_save_file)\u001b[0m\n\u001b[1;32m    677\u001b[0m \u001b[39m# -- Stop-gap implementation!\u001b[39;00m\n\u001b[1;32m    678\u001b[0m \u001b[39m#    fmin should have been a Trials method in the first place\u001b[39;00m\n\u001b[1;32m    679\u001b[0m \u001b[39m#    but for now it's still sitting in another file.\u001b[39;00m\n\u001b[1;32m    680\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mfmin\u001b[39;00m \u001b[39mimport\u001b[39;00m fmin\n\u001b[0;32m--> 682\u001b[0m \u001b[39mreturn\u001b[39;00m fmin(\n\u001b[1;32m    683\u001b[0m     fn,\n\u001b[1;32m    684\u001b[0m     space,\n\u001b[1;32m    685\u001b[0m     algo,\n\u001b[1;32m    686\u001b[0m     max_evals,\n\u001b[1;32m    687\u001b[0m     timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m    688\u001b[0m     loss_threshold\u001b[39m=\u001b[39;49mloss_threshold,\n\u001b[1;32m    689\u001b[0m     trials\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m,\n\u001b[1;32m    690\u001b[0m     rstate\u001b[39m=\u001b[39;49mrstate,\n\u001b[1;32m    691\u001b[0m     verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[1;32m    692\u001b[0m     max_queue_len\u001b[39m=\u001b[39;49mmax_queue_len,\n\u001b[1;32m    693\u001b[0m     allow_trials_fmin\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,  \u001b[39m# -- prevent recursion\u001b[39;49;00m\n\u001b[1;32m    694\u001b[0m     pass_expr_memo_ctrl\u001b[39m=\u001b[39;49mpass_expr_memo_ctrl,\n\u001b[1;32m    695\u001b[0m     catch_eval_exceptions\u001b[39m=\u001b[39;49mcatch_eval_exceptions,\n\u001b[1;32m    696\u001b[0m     return_argmin\u001b[39m=\u001b[39;49mreturn_argmin,\n\u001b[1;32m    697\u001b[0m     show_progressbar\u001b[39m=\u001b[39;49mshow_progressbar,\n\u001b[1;32m    698\u001b[0m     early_stop_fn\u001b[39m=\u001b[39;49mearly_stop_fn,\n\u001b[1;32m    699\u001b[0m     trials_save_file\u001b[39m=\u001b[39;49mtrials_save_file,\n\u001b[1;32m    700\u001b[0m )\n",
      "File \u001b[0;32m~/.virtualenvs/dissertacao/lib/python3.8/site-packages/hyperopt/fmin.py:553\u001b[0m, in \u001b[0;36mfmin\u001b[0;34m(fn, space, algo, max_evals, timeout, loss_threshold, trials, rstate, allow_trials_fmin, pass_expr_memo_ctrl, catch_eval_exceptions, verbose, return_argmin, points_to_evaluate, max_queue_len, show_progressbar, early_stop_fn, trials_save_file)\u001b[0m\n\u001b[1;32m    550\u001b[0m rval\u001b[39m.\u001b[39mcatch_eval_exceptions \u001b[39m=\u001b[39m catch_eval_exceptions\n\u001b[1;32m    552\u001b[0m \u001b[39m# next line is where the fmin is actually executed\u001b[39;00m\n\u001b[0;32m--> 553\u001b[0m rval\u001b[39m.\u001b[39;49mexhaust()\n\u001b[1;32m    555\u001b[0m \u001b[39mif\u001b[39;00m return_argmin:\n\u001b[1;32m    556\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(trials\u001b[39m.\u001b[39mtrials) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[0;32m~/.virtualenvs/dissertacao/lib/python3.8/site-packages/hyperopt/fmin.py:356\u001b[0m, in \u001b[0;36mFMinIter.exhaust\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    354\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mexhaust\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    355\u001b[0m     n_done \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrials)\n\u001b[0;32m--> 356\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrun(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_evals \u001b[39m-\u001b[39;49m n_done, block_until_done\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49masynchronous)\n\u001b[1;32m    357\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrials\u001b[39m.\u001b[39mrefresh()\n\u001b[1;32m    358\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[0;32m~/.virtualenvs/dissertacao/lib/python3.8/site-packages/hyperopt/fmin.py:292\u001b[0m, in \u001b[0;36mFMinIter.run\u001b[0;34m(self, N, block_until_done)\u001b[0m\n\u001b[1;32m    289\u001b[0m     time\u001b[39m.\u001b[39msleep(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpoll_interval_secs)\n\u001b[1;32m    290\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    291\u001b[0m     \u001b[39m# -- loop over trials and do the jobs directly\u001b[39;00m\n\u001b[0;32m--> 292\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mserial_evaluate()\n\u001b[1;32m    294\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrials\u001b[39m.\u001b[39mrefresh()\n\u001b[1;32m    295\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrials_save_file \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m:\n",
      "File \u001b[0;32m~/.virtualenvs/dissertacao/lib/python3.8/site-packages/hyperopt/fmin.py:170\u001b[0m, in \u001b[0;36mFMinIter.serial_evaluate\u001b[0;34m(self, N)\u001b[0m\n\u001b[1;32m    168\u001b[0m ctrl \u001b[39m=\u001b[39m base\u001b[39m.\u001b[39mCtrl(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrials, current_trial\u001b[39m=\u001b[39mtrial)\n\u001b[1;32m    169\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 170\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdomain\u001b[39m.\u001b[39;49mevaluate(spec, ctrl)\n\u001b[1;32m    171\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    172\u001b[0m     logger\u001b[39m.\u001b[39merror(\u001b[39m\"\u001b[39m\u001b[39mjob exception: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m \u001b[39mstr\u001b[39m(e))\n",
      "File \u001b[0;32m~/.virtualenvs/dissertacao/lib/python3.8/site-packages/hyperopt/base.py:907\u001b[0m, in \u001b[0;36mDomain.evaluate\u001b[0;34m(self, config, ctrl, attach_attachments)\u001b[0m\n\u001b[1;32m    898\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    899\u001b[0m     \u001b[39m# -- the \"work\" of evaluating `config` can be written\u001b[39;00m\n\u001b[1;32m    900\u001b[0m     \u001b[39m#    either into the pyll part (self.expr)\u001b[39;00m\n\u001b[1;32m    901\u001b[0m     \u001b[39m#    or the normal Python part (self.fn)\u001b[39;00m\n\u001b[1;32m    902\u001b[0m     pyll_rval \u001b[39m=\u001b[39m pyll\u001b[39m.\u001b[39mrec_eval(\n\u001b[1;32m    903\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexpr,\n\u001b[1;32m    904\u001b[0m         memo\u001b[39m=\u001b[39mmemo,\n\u001b[1;32m    905\u001b[0m         print_node_on_error\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrec_eval_print_node_on_error,\n\u001b[1;32m    906\u001b[0m     )\n\u001b[0;32m--> 907\u001b[0m     rval \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfn(pyll_rval)\n\u001b[1;32m    909\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(rval, (\u001b[39mfloat\u001b[39m, \u001b[39mint\u001b[39m, np\u001b[39m.\u001b[39mnumber)):\n\u001b[1;32m    910\u001b[0m     dict_rval \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mloss\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mfloat\u001b[39m(rval), \u001b[39m\"\u001b[39m\u001b[39mstatus\u001b[39m\u001b[39m\"\u001b[39m: STATUS_OK}\n",
      "File \u001b[0;32m~/elliot/elliot/hyperoptimization/model_coordinator.py:63\u001b[0m, in \u001b[0;36mModelCoordinator.objective\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlogger\u001b[39m.\u001b[39minfo(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mExploration: Train-Validation Fold exploration number \u001b[39m\u001b[39m{\u001b[39;00mtrainval_index\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     62\u001b[0m model \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_class(data\u001b[39m=\u001b[39mdata_obj, config\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbase, params\u001b[39m=\u001b[39mmodel_params)\n\u001b[0;32m---> 63\u001b[0m model\u001b[39m.\u001b[39;49mtrain()\n\u001b[1;32m     64\u001b[0m losses\u001b[39m.\u001b[39mappend(model\u001b[39m.\u001b[39mget_loss())\n\u001b[1;32m     65\u001b[0m results\u001b[39m.\u001b[39mappend(model\u001b[39m.\u001b[39mget_results())\n",
      "File \u001b[0;32m~/elliot/elliot/recommender/autoencoders/vae/multi_vae.py:99\u001b[0m, in \u001b[0;36mMultiVAE.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     97\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39m!!!!!!!!!!!!!!!!!!\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     98\u001b[0m     \u001b[39m###\u001b[39;00m\n\u001b[0;32m---> 99\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrestore_weights()\n\u001b[1;32m    100\u001b[0m \u001b[39m###\u001b[39;00m\n\u001b[1;32m    101\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39m!!!!!!!!!!!!!!!!!!\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/elliot/elliot/recommender/recommender_utils_mixin.py:107\u001b[0m, in \u001b[0;36mRecMixin.restore_weights\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    106\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m ex:\n\u001b[0;32m--> 107\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mException\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mError in model restoring operation! \u001b[39m\u001b[39m{\u001b[39;00mex\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    109\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n",
      "\u001b[0;31mException\u001b[0m: Error in model restoring operation! Unsuccessful TensorSliceReader constructor: Failed to find any matching files for /home/kpfra/streamRec-forgetting/notebooks/elliot_experiments/elliot_example/results/weights/MultiVAE_seed=42_e=3_bs=928_intermediate_dim=141_latent_dim=304_reg_lambda=0$4291070790802781_lr=0$09139786666274591_dropout_pkeep=0$3657655928728685/best-weights-MultiVAE_seed=42_e=3_bs=928_intermediate_dim=141_latent_dim=304_reg_lambda=0$4291070790802781_lr=0$09139786666274591_dropout_pkeep=0$3657655928728685"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-03-06 17:49:29.145544: I Exploration: Hyperparameter exploration number 1\n",
      "2023-03-06 17:49:29.147012: I Exploration: Test Fold exploration number 1\n",
      "2023-03-06 17:49:29.148684: I Exploration: Train-Validation Fold exploration number 1\n"
     ]
    }
   ],
   "source": [
    "run_experiment('elliot_example_configuration 2.yml')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dissertacao",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1587999827224b982432b08a1f15989203be23b6972d137605ee3addee06d4dd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
